{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "897f8abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both GOOGLE_API_KEY and GEMINI_API_KEY are set. Using GOOGLE_API_KEY.\n"
     ]
    }
   ],
   "source": [
    "#### this notebook allows u to cache the planning prompt.\n",
    "\n",
    "import os\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "from google import genai\n",
    "from prompts.granular_tooling_capabilities import GRANULAR_TOOLING_CAPABILITIES as prompt\n",
    "from ai_service_models import GranularPlanningResponse\n",
    "cache_name = None\n",
    "from pydantic import BaseModel\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional, Dict, Any, List\n",
    "from enum import Enum\n",
    "\n",
    "from google.genai import types\n",
    "\n",
    "client_gemini = genai.Client(api_key=GEMINI_API_KEY)\n",
    "function_granular_planning_response = types.FunctionDeclaration(\n",
    "    name='Create_Granular_Planning_Response',\n",
    "    description='Create a granular planning response for the user query, specifying exact tool function IDs needed.',\n",
    "    parameters_json_schema=GranularPlanningResponse.model_json_schema()\n",
    ")\n",
    "\n",
    "tool = types.Tool(function_declarations=[function_granular_planning_response])\n",
    "\n",
    "cache_type_prompt = client_gemini.caches.create(\n",
    "    model = \"gemini-2.5-flash\",\n",
    "    config=types.CreateCachedContentConfig(\n",
    "      display_name='type_prompt', # used to identify the cache\n",
    "      system_instruction=(\n",
    "        \"\"\"You are an expert task planner with deep knowledge of available tools.\n",
    "\n",
    "            **Your goal:** Analyze the user's request and determine:\n",
    "            1. **Query Type**: Is this a 'command' (task to execute) or 'conversational' (no action needed)?\n",
    "            2. **Tooling Need**: Does this require any tools, or can it be answered conversationally?\n",
    "            3. **Required Tools**: If tools are needed, specify EXACTLY which tool function IDs are required. Be precise and minimal.\n",
    "\n",
    "            **CRITICAL**: Only include tools that are ACTUALLY needed for THIS specific task. Don't include tools \"just in case.\" However, consider tools that need to be used in tandem to accomplish the task.\n",
    "\n",
    "            **IMPORTANT**: If multiple tools are needed, list them all and explain how they work together to complete the task.\n",
    "\n",
    "            the tooling capabilities are detailed in the system prompt.\n",
    "            Consider the conversation context. If a task was just completed, the user might be responding conversationally.\n",
    "            \"\"\"\n",
    "      ),\n",
    "      tools=[tool],\n",
    "      tool_config=types.ToolConfig(\n",
    "            function_calling_config=types.FunctionCallingConfig(mode='ANY')\n",
    "        ),\n",
    "\n",
    "      # response_mime_type='application/json',\n",
    "      # response_schema=TargetTypeAnnotation,\n",
    "      contents=[prompt],\n",
    "      ttl=\"300s\",  # 15 days\n",
    "  )\n",
    ")\n",
    "cache_name = cache_type_prompt.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "050c5726",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "\n",
    "planning_llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", google_api_key=GEMINI_API_KEY, thinking_budget=0, cached_content=cache_name)\n",
    "# planning_llm = planning_llm.with_structured_output(GranularPlanningResponse)\n",
    "response = planning_llm.invoke(\"Hi there\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74c33653",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cachedContents/b1f9auyk8vhn29yr3uwhy45q7bx8c9skvm0in418'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1f4688",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
