{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "897f8abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both GOOGLE_API_KEY and GEMINI_API_KEY are set. Using GOOGLE_API_KEY.\n"
     ]
    }
   ],
   "source": [
    "#### this notebook allows u to cache the planning prompt.\n",
    "import os\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "from google import genai\n",
    "from prompts.granular_tooling_capabilities import GRANULAR_TOOLING_CAPABILITIES as prompt\n",
    "from ai_service_models import GranularPlanningResponse\n",
    "cache_name = None\n",
    "from pydantic import BaseModel\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional, Dict, Any, List\n",
    "from enum import Enum\n",
    "from google.genai import types\n",
    "client_gemini = genai.Client(api_key=GEMINI_API_KEY)\n",
    "\n",
    "\n",
    "function_granular_planning_response = types.FunctionDeclaration(\n",
    "    name='Create_Granular_Planning_Response',\n",
    "    description='Create a granular planning response for the user query, specifying exact tool function IDs needed.',\n",
    "    parameters_json_schema=GranularPlanningResponse.model_json_schema()\n",
    ")\n",
    "\n",
    "tool = types.Tool(function_declarations=[function_granular_planning_response])\n",
    "\n",
    "cache_type_prompt = client_gemini.caches.create(\n",
    "    model = \"gemini-2.5-flash\",\n",
    "    config=types.CreateCachedContentConfig(\n",
    "      display_name='type_prompt', # used to identify the cache\n",
    "      system_instruction=(\n",
    "      \"\"\"\n",
    "        You are an expert task planner with deep knowledge of available tools.\n",
    "        **Your goal:** Analyze the user's request and determine:\n",
    "          1. **Query Type**: Is this a 'command' (task to execute) or 'conversational' (no action needed)?\n",
    "          2. **Tooling Need**: Does this require any tools, or can it be answered conversationally?\n",
    "          3. **Required Tools**: If tools are needed, specify EXACTLY which tool function IDs are required. Be precise and minimal. Specify them in order of use. \n",
    "          4. Use intermediate messaging tool to first send a confirmation message to the user when the task involves long operations (30+ seconds), such as browsing websites, identifying products in images, or generating videos. then, proceed with the main tool, and finally, use the appropriate messaging tool to send the final response.\n",
    "          **CRITICAL**: Only include tools that are ACTUALLY needed for THIS specific task. Don't include tools \"just in case.\" However, consider tools that need to be used in tandem to accomplish the task.\n",
    "          **IMPORTANT**: If multiple tools are needed, list them all and explain how they work together to complete the task.\n",
    "          **IMPORTANT**: we do not consider capabilities such as \"Transcribing the contents\" of an image, 'Translating the contents' of an email, 'transcribing an audio file', or 'summarizing a document' as separate tools. These are capabilities that are part of the core AI functionality, and do not require a separate tool. The tools listed here are for external integrations, or for specific actions that require a distinct function call. Such capabilities can be handled by the AI directly, without needing to invoke a separate tool. These capabilities are always available, and you can always do them. \n",
    "\n",
    "        the tooling capabilities are detailed in the system prompt.\n",
    "        Consider the conversation context. If a task was just completed, the user might be responding conversationally.\n",
    "      \"\"\"\n",
    "      ),\n",
    "      tools=[tool],\n",
    "      tool_config=types.ToolConfig(\n",
    "            function_calling_config=types.FunctionCallingConfig(mode='ANY')\n",
    "        ),\n",
    "\n",
    "      # response_mime_type='application/json',\n",
    "      # response_schema=TargetTypeAnnotation,\n",
    "      contents=[prompt],\n",
    "      ttl=\"1200s\",  # 15 days\n",
    "  )\n",
    ")\n",
    "cache_name = cache_type_prompt.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0f1a3e5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cachedContents/hmcssfuet0apskc4pu0lt3mkebg9er8anw96x0yd'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb158b9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050c5726",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'Create_Granular_Planning_Response',\n",
       "  'args': {'reason': 'The user is greeting the assistant. This is a conversational query and does not require any tools. The response will be handled conversationally.',\n",
       "   'query_type': 'conversational'},\n",
       "  'id': '2d6107af-b5eb-478e-b84e-e65d9eccc7ef',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "\n",
    "planning_llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", google_api_key=GEMINI_API_KEY, thinking_budget=0, cached_content=cache_name)\n",
    "# planning_llm = planning_llm.with_structured_output(GranularPlanningResponse)\n",
    "response = planning_llm.invoke(\"Hi there lil bro\")\n",
    "response.tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74c33653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# caches = client_gemini.caches.list()\n",
    "# for c in caches:\n",
    "#     client_gemini.caches.delete(name=c.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1f4688",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'Create_Granular_Planning_Response',\n",
       "  'args': {'query_type': 'command',\n",
       "   'steps': ['Generate an image of a flower using the `generate_image` tool.',\n",
       "    'Send the generated image to the user via WhatsApp using the `reply_to_user_on_whatsapp` tool.'],\n",
       "   'tooling_need': True,\n",
       "   'reason': 'The user wants to generate an image based on the provided description. The `generate_image` tool is suitable for this purpose. After generating the image, the `reply_to_user_on_whatsapp` tool will be used to send the image to the user via WhatsApp.',\n",
       "   'required_tools': ['generate_image', 'reply_to_user_on_whatsapp']},\n",
       "  'id': 'c0325adf-1cba-49b6-936c-9702f51fa100',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f75121",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
